{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda9elkh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from environments import Hex\n",
    "from networks import CNN, ANN\n",
    "from mcts import MCTS\n",
    "from agents import MCTSAgent, ANNAgent, CNNAgent\n",
    "from memory import Memory\n",
    "from misc import LiteModel\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"testing.ipynb\"\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    filename = \"/Users/daniel/Documents/AIProg/Assignments/Assignment 2/cases/r_1500_mcts/train_samples\"\n",
    "    states = np.loadtxt(filename + '_states.txt', dtype=np.int32)\n",
    "    dists = np.loadtxt(filename + '_dists.txt', dtype=np.float32)\n",
    "\n",
    "    bits = lambda s: format(s if s == 1 else 2, f\"0{2}b\")\n",
    "    new_states = np.zeros((states.shape[0], 100))\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        new_states[i] = np.array([float(s) for s in list(''.join([bits(s) for s in states[i]]))])\n",
    "\n",
    "    return new_states.astype(np.float32), dists\n",
    "\n",
    "states, dists = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Hex(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "annagent = ANNAgent(network=ann)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "\n",
    "## PLOTTING\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Loss\")\n",
    "## /PLOTTING\n",
    "\n",
    "decay = 0.05**(1/1000)\n",
    "i=0\n",
    "\n",
    "def save_model():\n",
    "    ann.save_model(suffix=f\"S{env.size}_B{i}\")\n",
    "\n",
    "save_model()\n",
    "\n",
    "wandb.init(project=\"hex\", config={\"mcts\": mcts.config, \"ann\": ann.config})\n",
    "\n",
    "while True:\n",
    "    env.reset()\n",
    "    while not env.is_game_over:\n",
    "        best_move, distribution = agent.get_move(greedy=False)\n",
    "        memory.register(\"player\", env.current_player)\n",
    "        memory.register(\"action\", best_move)\n",
    "        memory.register(\"state\", env.ann_state)\n",
    "        memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "        env.play(best_move)\n",
    "\n",
    "    memory.register_result(env.result)\n",
    "\n",
    "    samples = memory.all()\n",
    "    result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "    i += 1\n",
    "\n",
    "    wandb.log({\"accuracy\": result[\"accuracy\"], \"loss\": result[\"loss\"], \"epsilon\": mcts.epsilon})\n",
    "    mcts.epsilon *= decay\n",
    "\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Hex(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "cnn = CNN.build(learning_rate=0.001, input_shape=env.cnn_state.shape, output_size=len(env.legal_binary_moves), hidden_layers=(32,32,32,32,32,32,32), optimizer=\"adam\", activation=\"sigmoid\")\n",
    "annagent = ANNAgent(network=ann)\n",
    "cnnagent = CNNAgent(network=cnn)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=5000, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential \n",
    "model = Sequential.from_config(cnn.model.get_config())\n",
    "model.set_weights(cnn.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = CNNAgent(environment=env, network=CNN(model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1),\n",
       " array([0.05649541, 0.06272533, 0.02742418, 0.03721538, 0.05506705,\n",
       "        0.14963758, 0.02485981, 0.03450555, 0.05404091, 0.04521538,\n",
       "        0.05875241, 0.05662227, 0.09146364, 0.03520534, 0.10622849,\n",
       "        0.10454122], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.current_player = -1\n",
    "agent1.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1),\n",
       " array([0.05649541, 0.06272533, 0.02742418, 0.03721538, 0.05506705,\n",
       "        0.14963758, 0.02485981, 0.03450555, 0.05404091, 0.04521538,\n",
       "        0.05875241, 0.05662227, 0.09146364, 0.03520534, 0.10622849,\n",
       "        0.10454122], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnagent.environment = env\n",
    "cnnagent.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:01:38.302091: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmptej06gg9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 13:01:47.032335: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-04 13:01:47.032354: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "2022-04-04 13:01:47.033076: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmptej06gg9\n",
      "2022-04-04 13:01:47.039858: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-04 13:01:47.039881: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmptej06gg9\n",
      "2022-04-04 13:01:47.071669: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2022-04-04 13:01:47.162540: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmptej06gg9\n",
      "2022-04-04 13:01:47.202205: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 169132 microseconds.\n",
      "2022-04-04 13:01:47.278654: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1, 1),\n",
       " array([0.05649541, 0.06272533, 0.02742418, 0.03721538, 0.05506705,\n",
       "        0.14963761, 0.02485981, 0.03450554, 0.05404091, 0.04521537,\n",
       "        0.05875241, 0.05662228, 0.09146363, 0.03520533, 0.10622849,\n",
       "        0.10454122], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel = LiteModel.from_keras_model(model)\n",
    "agent2 = CNNAgent(environment=env, network=CNN(model=lmodel))\n",
    "agent2.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ray.remote\n",
    "class ANNTrainer:\n",
    "    def __initialize__(self, memory):\n",
    "        self.memory = memory\n",
    "        self.ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "\n",
    "    def train():\n",
    "        result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class ANETPredictor:\n",
    "    def __initialize__(self, storage):\n",
    "        pass\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MCTSWorker:\n",
    "    def __initialize__(self, memory, anet_config):\n",
    "        self.environment = Hex(size=4)\n",
    "        self.model = Sequential.from_config(anet_config)\n",
    "        self.memory = memory\n",
    "\n",
    "    def simulate(self):\n",
    "        self.set_weights()\n",
    "        self.environment.reset()\n",
    "        while not env.is_game_over:\n",
    "            best_move, distribution = agent.get_move(greedy=False)\n",
    "            \n",
    "            memory.register(\"player\", self.environment.current_player)\n",
    "            memory.register(\"action\", best_move)\n",
    "            memory.register(\"state\", self.environment.ann_state)\n",
    "            memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "            \n",
    "            self.environment.play(best_move)\n",
    "\n",
    "        memory.register_result(env.result)\n",
    "        pass\n",
    "\n",
    "    def set_weights(self):\n",
    "        self.model.set_weights(ray.get(storage)[\"weights\"])\n",
    "\n",
    "\n",
    "# Memory\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "ray_memory = ray.put(memory)\n",
    "\n",
    "# Workers\n",
    "trainer = ANNTrainer.remote(memory)\n",
    "mcts_workers = [MCTSWorker.remote(memory, anet_config) for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a complete solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()\n",
    "\n",
    "import numpy as np\n",
    "from environments import Hex\n",
    "from networks import ANN\n",
    "from mcts import MCTS\n",
    "from agents import MCTSAgent, ANNAgent\n",
    "from misc import LiteModel\n",
    "\n",
    "class GameHistory:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        self.players = []\n",
    "        self.states = []\n",
    "        self.distributions = []\n",
    "        self.moves = []\n",
    "\n",
    "    def register_result(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def register_move(self, player, move, state, distribution):\n",
    "        self.players.append(player)\n",
    "        self.states.append(state)\n",
    "        self.moves.append(move)\n",
    "        self.distributions.append(distribution)\n",
    "\n",
    "    def stack(self):\n",
    "        memory = []\n",
    "        for i in range(len(self.states)):\n",
    "            player = self.players[i]\n",
    "            move = self.moves[i]   \n",
    "            state = self.states[i]\n",
    "            distribution = self.distributions[i]\n",
    "            result = self.result            \n",
    "            memory.append([player, move, state, distribution, result])\n",
    "        return memory\n",
    "\n",
    "@ray.remote\n",
    "class Storage:\n",
    "    def __init__(self):\n",
    "        self.data = { \"terminate\": False }\n",
    "\n",
    "    def get_info(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    def set_info(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def all(self):\n",
    "        return self.data\n",
    "\n",
    "@ray.remote\n",
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        self.num_games = 0\n",
    "        self.num_samples = 0\n",
    "\n",
    "    def store(self, game_history): # Game history\n",
    "        self.buffer[self.num_games] = game_history\n",
    "        self.num_games += 1\n",
    "        self.num_samples += len(game_history.states)\n",
    "        \n",
    "    def get_batch(self, sample_size):\n",
    "        keys = random.choice(list(self.buffer.keys()), sample_size)\n",
    "        returns = [self.buffer[key] for key in keys]\n",
    "        return returns\n",
    "\n",
    "    def length(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.initialized = False\n",
    "\n",
    "    def loop(self, storage, buffer):\n",
    "        if not self.initialized:\n",
    "            self.initialize_ann(storage)\n",
    "        \n",
    "        while not (ray.get(storage.get_info.remote(\"terminate\")) and ray.get(storage.get_info.remote(\"num_played_games\")) < 1):\n",
    "            histories = ray.get(buffer.get_batch.remote(256))\n",
    "\n",
    "            states = np.array([history.states for history in histories])\n",
    "            distributions = np.array([history.distributions for history in histories])\n",
    "\n",
    "            results = self.network.train_on_batch(states, distributions)\n",
    "\n",
    "            weights = self.network.model.get_weights()\n",
    "            storage.set_info(\"weights\", weights)\n",
    "\n",
    "    def initialize_ann(self, storage):\n",
    "        weights = self.network.model.get_weights()\n",
    "        config = self.network.model.get_config()\n",
    "        storage.set_info(\"nn_weights\", weights)\n",
    "        storage.set_info(\"nn_config\", config)\n",
    "        self.initialized = True\n",
    "\n",
    "    def store(self):\n",
    "        pass\n",
    "        # Save model\n",
    "        \n",
    "\n",
    "@ray.remote\n",
    "class MCTSWorker:\n",
    "    def __init__(self, model):\n",
    "        self.initialized = False\n",
    "        self.environment = Hex(size=4)\n",
    "        self.model = LiteModel.from_keras_model(model)\n",
    "        self.network = ANN(model=self.model)\n",
    "        self.ann_agent = ANNAgent(environment=self.environment, network=self.network)\n",
    "        self.mcts = MCTS(environment=env, rollout_policy_agent=self.ann_agent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "        self.agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "    \n",
    "    def update_weights(self, storage):\n",
    "        weights = ray.get(storage.get_info.remote(\"nn_weights\"))\n",
    "        config = ray.get(storage.get_info.remote(\"nn_config\"))\n",
    "\n",
    "        seq_model = Sequential.from_config(config)\n",
    "        seq_model.set_weights(weights)\n",
    "\n",
    "        self.model.update_keras_model(seq_model)\n",
    "\n",
    "    def loop(self, storage, buffer):\n",
    "        while not ray.get(storage.get_info.remote(\"terminate\")):\n",
    "            self.update_weights(storage)\n",
    "            self.environment.reset()\n",
    "            \n",
    "            gh = GameHistory()\n",
    "\n",
    "            while not self.enviroment.is_game_over:\n",
    "                move, distribution = self.agent.get_move(greedy=False)\n",
    "\n",
    "                # Store move information in buffer\n",
    "                gh.register_move(self.environment.current_player, move, self.environment.flat_state, distribution)\n",
    "\n",
    "                self.environment.play(move)\n",
    "\n",
    "            buffer.store.remote(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STARTING HERE ####\n",
    "network = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "\n",
    "buffer = Buffer.remote()\n",
    "storage = Storage.remote()\n",
    "trainer = Trainer.remote(network)\n",
    "workers = [MCTSWorker.remote(network.model) for _ in range(2)]\n",
    "\n",
    "\n",
    "# Run loops\n",
    "trainer.loop.remote(storage, buffer)\n",
    "for worker in mcts_workers:\n",
    "    worker.loop.remote(storage, buffer)\n",
    "\n",
    "while True:\n",
    "    print(ray.get(buffer.length.remote()))\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3756f3ae31dbc227d61eaa7f41edcb2155d167bc20398bc830f506f256990ec2"
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
