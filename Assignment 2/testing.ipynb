{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda9elkh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from environments import HexGame\n",
    "from networks import CNN, ANN\n",
    "from mcts import MCTS\n",
    "from agents import MCTSAgent, ANNAgent\n",
    "from memory import Memory\n",
    "from misc import LiteModel\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"testing.ipynb\"\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    filename = \"/Users/daniel/Documents/AIProg/Assignments/Assignment 2/cases/r_1500_mcts/train_samples\"\n",
    "    states = np.loadtxt(filename + '_states.txt', dtype=np.int32)\n",
    "    dists = np.loadtxt(filename + '_dists.txt', dtype=np.float32)\n",
    "\n",
    "    bits = lambda s: format(s if s == 1 else 2, f\"0{2}b\")\n",
    "    new_states = np.zeros((states.shape[0], 100))\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        new_states[i] = np.array([float(s) for s in list(''.join([bits(s) for s in states[i]]))])\n",
    "\n",
    "    return new_states.astype(np.float32), dists\n",
    "\n",
    "states, dists = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HexGame(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "annagent = ANNAgent(network=ann)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "\n",
    "## PLOTTING\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Loss\")\n",
    "## /PLOTTING\n",
    "\n",
    "decay = 0.05**(1/1000)\n",
    "i=0\n",
    "\n",
    "def save_model():\n",
    "    ann.save_model(suffix=f\"S{env.size}_B{i}\")\n",
    "\n",
    "save_model()\n",
    "\n",
    "wandb.init(project=\"hex\", config={\"mcts\": mcts.config, \"ann\": ann.config})\n",
    "\n",
    "while True:\n",
    "    env.reset()\n",
    "    while not env.is_game_over:\n",
    "        best_move, distribution = agent.get_move(greedy=False)\n",
    "        memory.register(\"player\", env.current_player)\n",
    "        memory.register(\"action\", best_move)\n",
    "        memory.register(\"state\", env.ann_state)\n",
    "        memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "        env.play(best_move)\n",
    "\n",
    "    memory.register_result(env.result)\n",
    "\n",
    "    samples = memory.all()\n",
    "    result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "    i += 1\n",
    "\n",
    "    wandb.log({\"accuracy\": result[\"accuracy\"], \"loss\": result[\"loss\"], \"epsilon\": mcts.epsilon})\n",
    "    mcts.epsilon *= decay\n",
    "\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 23:26:07.061644: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "env = HexGame(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "annagent = ANNAgent(network=ann)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential.from_config(ann.model.get_config())\n",
    "model.set_weights(ann.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = ANNAgent(environment=env, network=ANN(model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1),\n",
       " array([0.06404223, 0.06711077, 0.06206467, 0.0631833 , 0.06142657,\n",
       "        0.06087947, 0.05806388, 0.06501137, 0.06426103, 0.06147993,\n",
       "        0.06528415, 0.06341712, 0.05925861, 0.06046248, 0.06593056,\n",
       "        0.05812385], dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 1),\n",
       " array([0.06404223, 0.06711077, 0.06206467, 0.0631833 , 0.06142657,\n",
       "        0.06087947, 0.05806388, 0.06501137, 0.06426103, 0.06147993,\n",
       "        0.06528415, 0.06341712, 0.05925861, 0.06046248, 0.06593056,\n",
       "        0.05812385], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annagent.environment = env\n",
    "annagent.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpevupc_2p/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpevupc_2p/assets\n",
      "2022-04-03 23:26:56.521302: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-03 23:26:56.521321: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-03 23:26:56.521433: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpevupc_2p\n",
      "2022-04-03 23:26:56.522769: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-03 23:26:56.522794: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpevupc_2p\n",
      "2022-04-03 23:26:56.528201: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2022-04-03 23:26:56.549386: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpevupc_2p\n",
      "2022-04-03 23:26:56.559994: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 38561 microseconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0, 1),\n",
       " array([0.06404223, 0.06711077, 0.06206467, 0.0631833 , 0.06142657,\n",
       "        0.06087947, 0.05806388, 0.06501137, 0.06426103, 0.06147994,\n",
       "        0.06528415, 0.06341712, 0.05925861, 0.06046248, 0.06593056,\n",
       "        0.05812385], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmodel = LiteModel.from_keras_model(model)\n",
    "agent2 = ANNAgent(environment=env, network=ANN(model=lmodel))\n",
    "agent2.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/AIProg/Assignments/Assignment 2/testing.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/AIProg/Assignments/Assignment%202/testing.ipynb#ch0000033?line=0'>1</a>\u001b[0m agent\u001b[39m.\u001b[39;49mget_move(greedy\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/AIProg/Assignments/Assignment 2/agents/agent.py:15\u001b[0m, in \u001b[0;36mAgent.get_move\u001b[0;34m(self, greedy)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_move\u001b[39m(\u001b[39mself\u001b[39m, greedy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=14'>15</a>\u001b[0m     distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribution\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m greedy:\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=17'>18</a>\u001b[0m         value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(distribution)\n",
      "File \u001b[0;32m~/Documents/AIProg/Assignments/Assignment 2/agents/ann_agent.py:14\u001b[0m, in \u001b[0;36mANNAgent.distribution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=11'>12</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=13'>14</a>\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvironment\u001b[39m.\u001b[39;49mann_state]))[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=14'>15</a>\u001b[0m     dist \u001b[39m=\u001b[39m dist \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mlegal_binary_moves\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=15'>16</a>\u001b[0m     dist \u001b[39m=\u001b[39m dist \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(dist)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ray.remote\n",
    "class ANNTrainer:\n",
    "    def __initialize__(self, memory):\n",
    "        self.memory = memory\n",
    "        self.ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "\n",
    "    def train():\n",
    "        result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class ANETPredictor:\n",
    "    def __initialize__(self, storage):\n",
    "        pass\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MCTSWorker:\n",
    "    def __initialize__(self, memory, anet_config):\n",
    "        self.environment = HexGame(size=4)\n",
    "        self.model = Sequential.from_config(anet_config)\n",
    "        self.memory = memory\n",
    "\n",
    "    def simulate(self):\n",
    "        self.set_weights()\n",
    "        self.environment.reset()\n",
    "        while not env.is_game_over:\n",
    "            best_move, distribution = agent.get_move(greedy=False)\n",
    "            \n",
    "            memory.register(\"player\", self.environment.current_player)\n",
    "            memory.register(\"action\", best_move)\n",
    "            memory.register(\"state\", self.environment.ann_state)\n",
    "            memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "            \n",
    "            self.environment.play(best_move)\n",
    "\n",
    "        memory.register_result(env.result)\n",
    "        pass\n",
    "\n",
    "    def set_weights(self):\n",
    "        self.model.set_weights(ray.get(storage)[\"weights\"])\n",
    "\n",
    "\n",
    "# Memory\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "ray_memory = ray.put(memory)\n",
    "\n",
    "# Workers\n",
    "trainer = ANNTrainer.remote(memory)\n",
    "mcts_workers = [MCTSWorker.remote(memory, anet_config) for i in range(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a complete solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init()\n",
    "\n",
    "import numpy as np\n",
    "from environments import HexGame\n",
    "from networks import ANN\n",
    "from mcts import MCTS\n",
    "from agents import MCTSAgent, ANNAgent\n",
    "from misc import LiteModel\n",
    "\n",
    "class GameHistory:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        self.players = []\n",
    "        self.states = []\n",
    "        self.distributions = []\n",
    "        self.moves = []\n",
    "\n",
    "    def register_result(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def register_move(self, player, move, state, distribution):\n",
    "        self.players.append(player)\n",
    "        self.states.append(state)\n",
    "        self.moves.append(move)\n",
    "        self.distributions.append(distribution)\n",
    "\n",
    "    def stack(self):\n",
    "        memory = []\n",
    "        for i in range(len(self.states)):\n",
    "            player = self.players[i]\n",
    "            move = self.moves[i]   \n",
    "            state = self.states[i]\n",
    "            distribution = self.distributions[i]\n",
    "            result = self.result            \n",
    "            memory.append([player, move, state, distribution, result])\n",
    "        return memory\n",
    "\n",
    "@ray.remote\n",
    "class Storage:\n",
    "    def __init__(self):\n",
    "        self.data = { \"terminate\": False }\n",
    "\n",
    "    def get_info(self, key):\n",
    "        return self.data[key]\n",
    "\n",
    "    def set_info(self, key, value):\n",
    "        self.data[key] = value\n",
    "\n",
    "    def all(self):\n",
    "        return self.data\n",
    "\n",
    "@ray.remote\n",
    "class Buffer:\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        self.num_games = 0\n",
    "        self.num_samples = 0\n",
    "\n",
    "    def store(self, game_history): # Game history\n",
    "        self.buffer[self.num_games] = game_history\n",
    "        self.num_games += 1\n",
    "        self.num_samples += len(game_history.states)\n",
    "        \n",
    "    def get_batch(self, sample_size):\n",
    "        keys = random.choice(list(self.buffer.keys()), sample_size)\n",
    "        returns = [self.buffer[key] for key in keys]\n",
    "        return returns\n",
    "\n",
    "    def length(self):\n",
    "        return self.num_samples\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Trainer:\n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.initialized = False\n",
    "\n",
    "    def loop(self, storage, buffer):\n",
    "        if not self.initialized:\n",
    "            self.initialize_ann(storage)\n",
    "        \n",
    "        while not (ray.get(storage.get_info.remote(\"terminate\")) and ray.get(storage.get_info.remote(\"num_played_games\")) < 1):\n",
    "            histories = ray.get(buffer.get_batch.remote(256))\n",
    "\n",
    "            states = np.array([history.states for history in histories])\n",
    "            distributions = np.array([history.distributions for history in histories])\n",
    "\n",
    "            results = self.network.train_on_batch(states, distributions)\n",
    "\n",
    "            weights = self.network.model.get_weights()\n",
    "            storage.set_info(\"weights\", weights)\n",
    "\n",
    "    def initialize_ann(self, storage):\n",
    "        weights = self.network.model.get_weights()\n",
    "        config = self.network.model.get_config()\n",
    "        storage.set_info(\"nn_weights\", weights)\n",
    "        storage.set_info(\"nn_config\", config)\n",
    "        self.initialized = True\n",
    "\n",
    "    def store(self):\n",
    "        pass\n",
    "        # Save model\n",
    "        \n",
    "\n",
    "@ray.remote\n",
    "class MCTSWorker:\n",
    "    def __init__(self, model):\n",
    "        self.initialized = False\n",
    "        self.environment = HexGame(size=4)\n",
    "        self.model = LiteModel.from_keras_model(model)\n",
    "        self.network = ANN(model=self.model)\n",
    "        self.ann_agent = ANNAgent(environment=self.environment, network=self.network)\n",
    "        self.mcts = MCTS(environment=env, rollout_policy_agent=self.ann_agent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "        self.agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "    \n",
    "    def update_weights(self, storage):\n",
    "        weights = ray.get(storage.get_info.remote(\"nn_weights\"))\n",
    "        config = ray.get(storage.get_info.remote(\"nn_config\"))\n",
    "\n",
    "        seq_model = Sequential.from_config(config)\n",
    "        seq_model.set_weights(weights)\n",
    "\n",
    "        self.model.update_keras_model(seq_model)\n",
    "\n",
    "    def loop(self, storage, buffer):\n",
    "        while not ray.get(storage.get_info.remote(\"terminate\")):\n",
    "            self.update_weights(storage)\n",
    "            self.environment.reset()\n",
    "            \n",
    "            gh = GameHistory()\n",
    "\n",
    "            while not self.enviroment.is_game_over:\n",
    "                move, distribution = self.agent.get_move(greedy=False)\n",
    "\n",
    "                # Store move information in buffer\n",
    "                gh.register_move(self.environment.current_player, move, self.environment.flat_state, distribution)\n",
    "\n",
    "                self.environment.play(move)\n",
    "\n",
    "            buffer.store.remote(gh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STARTING HERE ####\n",
    "network = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "\n",
    "buffer = Buffer.remote()\n",
    "storage = Storage.remote()\n",
    "trainer = Trainer.remote(network)\n",
    "workers = [MCTSWorker.remote(network.model) for _ in range(2)]\n",
    "\n",
    "\n",
    "# Run loops\n",
    "trainer.loop.remote(storage, buffer)\n",
    "for worker in mcts_workers:\n",
    "    worker.loop.remote(storage, buffer)\n",
    "\n",
    "while True:\n",
    "    print(ray.get(buffer.length.remote()))\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3756f3ae31dbc227d61eaa7f41edcb2155d167bc20398bc830f506f256990ec2"
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
