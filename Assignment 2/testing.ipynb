{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda9elkh\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from environments import HexGame\n",
    "from networks import CNN, ANN\n",
    "from mcts import MCTS\n",
    "from agents import MCTSAgent, ANNAgent\n",
    "from memory import Memory\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"testing.ipynb\"\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    filename = \"/Users/daniel/Documents/AIProg/Assignments/Assignment 2/cases/r_1500_mcts/train_samples\"\n",
    "    states = np.loadtxt(filename + '_states.txt', dtype=np.int32)\n",
    "    dists = np.loadtxt(filename + '_dists.txt', dtype=np.float32)\n",
    "\n",
    "    bits = lambda s: format(s if s == 1 else 2, f\"0{2}b\")\n",
    "    new_states = np.zeros((states.shape[0], 100))\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        new_states[i] = np.array([float(s) for s in list(''.join([bits(s) for s in states[i]]))])\n",
    "\n",
    "    return new_states.astype(np.float32), dists\n",
    "\n",
    "states, dists = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = HexGame(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "annagent = ANNAgent(network=ann)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "\n",
    "## PLOTTING\n",
    "train_accuracies = []\n",
    "train_loss = []\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Accuracy\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Loss\")\n",
    "## /PLOTTING\n",
    "\n",
    "decay = 0.05**(1/1000)\n",
    "i=0\n",
    "\n",
    "def save_model():\n",
    "    ann.save_model(suffix=f\"S{env.size}_B{i}\")\n",
    "\n",
    "save_model()\n",
    "\n",
    "wandb.init(project=\"hex\", config={\"mcts\": mcts.config, \"ann\": ann.config})\n",
    "\n",
    "while True:\n",
    "    env.reset()\n",
    "    while not env.is_game_over:\n",
    "        best_move, distribution = agent.get_move(greedy=False)\n",
    "        memory.register(\"player\", env.current_player)\n",
    "        memory.register(\"action\", best_move)\n",
    "        memory.register(\"state\", env.ann_state)\n",
    "        memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "        env.play(best_move)\n",
    "\n",
    "    memory.register_result(env.result)\n",
    "\n",
    "    samples = memory.all()\n",
    "    result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "    i += 1\n",
    "\n",
    "    wandb.log({\"accuracy\": result[\"accuracy\"], \"loss\": result[\"loss\"], \"epsilon\": mcts.epsilon})\n",
    "    mcts.epsilon *= decay\n",
    "\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 18:13:29.719788: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "env = HexGame(size=4)\n",
    "ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "annagent = ANNAgent(network=ann)\n",
    "mcts = MCTS(environment=env, rollout_policy_agent=annagent, use_time_budget=False, rollouts=1500, c=1.4, epsilon=1)\n",
    "agent = MCTSAgent(environment=env, mcts=mcts)\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LiteModel:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "    \n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        return self.predict(inp)\n",
    "\n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "    \n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential.from_config(ann.model.get_config())\n",
    "model.set_weights(ann.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1 = ANNAgent(environment=env, network=ANN(model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3),\n",
       " array([0.0650397 , 0.06574406, 0.06357822, 0.05789192, 0.05793533,\n",
       "        0.06129165, 0.06438028, 0.06808408, 0.05823367, 0.06183377,\n",
       "        0.06072819, 0.06303644, 0.06495878, 0.05859031, 0.06064562,\n",
       "        0.06802795], dtype=float32))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3),\n",
       " array([0.0650397 , 0.06574406, 0.06357822, 0.05789192, 0.05793533,\n",
       "        0.06129165, 0.06438028, 0.06808408, 0.05823367, 0.06183377,\n",
       "        0.06072819, 0.06303644, 0.06495878, 0.05859031, 0.06064562,\n",
       "        0.06802795], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annagent.environment = env\n",
    "annagent.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpbo1jyy5k/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpbo1jyy5k/assets\n",
      "2022-04-03 18:26:02.109114: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:363] Ignored output_format.\n",
      "2022-04-03 18:26:02.109134: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:366] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n",
      "2022-04-03 18:26:02.109296: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpbo1jyy5k\n",
      "2022-04-03 18:26:02.110264: I tensorflow/cc/saved_model/reader.cc:107] Reading meta graph with tags { serve }\n",
      "2022-04-03 18:26:02.110276: I tensorflow/cc/saved_model/reader.cc:148] Reading SavedModel debug info (if present) from: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpbo1jyy5k\n",
      "2022-04-03 18:26:02.114246: I tensorflow/cc/saved_model/loader.cc:210] Restoring SavedModel bundle.\n",
      "2022-04-03 18:26:02.136176: I tensorflow/cc/saved_model/loader.cc:194] Running initialization op on SavedModel bundle at path: /var/folders/6t/6j6cdw090wn8tw74l39_zy_00000gp/T/tmpbo1jyy5k\n",
      "2022-04-03 18:26:02.144937: I tensorflow/cc/saved_model/loader.cc:283] SavedModel load for tags { serve }; Status: success: OK. Took 35642 microseconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/AIProg/Assignments/Assignment 2/testing.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/AIProg/Assignments/Assignment%202/testing.ipynb#ch0000015?line=0'>1</a>\u001b[0m lmodel \u001b[39m=\u001b[39m LiteModel\u001b[39m.\u001b[39mfrom_keras_model(model)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/daniel/Documents/AIProg/Assignments/Assignment%202/testing.ipynb#ch0000015?line=1'>2</a>\u001b[0m lagent \u001b[39m=\u001b[39m ANNAgent(environment\u001b[39m=\u001b[39menv, network\u001b[39m=\u001b[39mANN(model\u001b[39m=\u001b[39mlmodel))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/daniel/Documents/AIProg/Assignments/Assignment%202/testing.ipynb#ch0000015?line=2'>3</a>\u001b[0m lagent\u001b[39m.\u001b[39;49mget_move(greedy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/AIProg/Assignments/Assignment 2/agents/agent.py:15\u001b[0m, in \u001b[0;36mAgent.get_move\u001b[0;34m(self, greedy)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_move\u001b[39m(\u001b[39mself\u001b[39m, greedy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=14'>15</a>\u001b[0m     distribution \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribution\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m greedy:\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/agent.py?line=17'>18</a>\u001b[0m         value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(distribution)\n",
      "File \u001b[0;32m~/Documents/AIProg/Assignments/Assignment 2/agents/ann_agent.py:14\u001b[0m, in \u001b[0;36mANNAgent.distribution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=11'>12</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=13'>14</a>\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49marray([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvironment\u001b[39m.\u001b[39;49mann_state]))[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=14'>15</a>\u001b[0m     dist \u001b[39m=\u001b[39m dist \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment\u001b[39m.\u001b[39mlegal_binary_moves\n\u001b[1;32m     <a href='file:///Users/daniel/Documents/AIProg/Assignments/Assignment%202/agents/ann_agent.py?line=15'>16</a>\u001b[0m     dist \u001b[39m=\u001b[39m dist \u001b[39m/\u001b[39m \u001b[39msum\u001b[39m(dist)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "lmodel = LiteModel.from_keras_model(model)\n",
    "lagent = ANNAgent(environment=env, network=ANN(model=lmodel))\n",
    "lagent.get_move(greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@ray.remote\n",
    "class ANNTrainer:\n",
    "    def __initialize__(self, memory):\n",
    "        self.memory = memory\n",
    "        self.ann = ANN.build(learning_rate=0.001, input_size=len(env.ann_state), output_size=len(env.legal_binary_moves), activation=\"relu\", optimizer=\"adam\", hidden_size=(100, 100))\n",
    "\n",
    "    def train():\n",
    "        result = ann.train_on_batch(samples[2], samples[3], samples[4])\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class ANETPredictor:\n",
    "    def __initialize__(self, storage):\n",
    "        pass\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class MCTSWorker:\n",
    "    def __initialize__(self, memory, anet_config):\n",
    "        self.environment = HexGame(size=4)\n",
    "        self.model = Sequential.from_config(anet_config)\n",
    "        self.memory = memory\n",
    "\n",
    "    def simulate(self):\n",
    "        self.set_weights()\n",
    "        self.environment.reset()\n",
    "        while not env.is_game_over:\n",
    "            best_move, distribution = agent.get_move(greedy=False)\n",
    "            \n",
    "            memory.register(\"player\", self.environment.current_player)\n",
    "            memory.register(\"action\", best_move)\n",
    "            memory.register(\"state\", self.environment.ann_state)\n",
    "            memory.register(\"distribution\", distribution.flatten().tolist())\n",
    "            \n",
    "            self.environment.play(best_move)\n",
    "\n",
    "        memory.register_result(env.result)\n",
    "        pass\n",
    "\n",
    "    def set_weights(self):\n",
    "        self.model.set_weights(ray.get(storage)[\"weights\"])\n",
    "\n",
    "\n",
    "# Memory\n",
    "memory = Memory(sample_size=1.0, queue_size=10000, verbose=False)\n",
    "ray_memory = ray.put(memory)\n",
    "\n",
    "# Workers\n",
    "trainer = ANNTrainer.remote(memory)\n",
    "mcts_workers = [MCTSWorker.remote(memory, anet_config) for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "@ray.remote\n",
    "class Predictor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def loop(self):\n",
    "        pass\n",
    "\n",
    "    def wait(self, state):\n",
    "\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class Workers:\n",
    "    def __init__(self, pred):\n",
    "        self.predictor = pred\n",
    "\n",
    "    def train(self):\n",
    "        time.sleep(random.randint(0,6))\n",
    "        self.predictor.wait(id(self), 10)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ray\n",
    "\n",
    "def function(test):\n",
    "    time.sleep(4)\n",
    "    return test\n",
    "\n",
    "ray.init()\n",
    "test = function.remote(\"hei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15743726905844901"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.wait(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3756f3ae31dbc227d61eaa7f41edcb2155d167bc20398bc830f506f256990ec2"
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
