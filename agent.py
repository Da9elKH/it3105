from environments.cart_pole import CartPole
from environments.tower_of_hanoi import TowerOfHanoi
from environments.the_gambler import TheGambler
from environments.environment import ProblemEnvironment
from critics.critic import Critic
from critics.nn import NeuralNetworkCritic
from utils.decaying_variable import DecayingVariable
from critics.table import TableCritic
from actor import Actor
from enum import Enum


class GameType(Enum):
    CART_POLE = 1
    TOWER_OF_HANOI = 2
    GAMBLER = 3


class CriticType(Enum):
    TABLE = 1
    NEURAL_NETWORK = 2

import random

class Agent:
    def __init__(self, env: ProblemEnvironment, act: Actor, crt: Critic):
        self.env = env
        self.act = act
        self.crt = crt

    def actor_critic_model(self, episodes: int):
        win = 0

        for episode in range(episodes):
            # Reset eligibilities for actor and critic
            self.crt.clear()
            self.act.clear()

            # Initialize environment
            state, valid_actions = self.env.reset()

            # Helpers for replay logic
            last_episode = episode == (episodes - 1)
            i = 0

            # Store data for later visualisation
            self.env.store_training_metadata(current_episode=episode, last_episode=last_episode, current_step=i, state=state)

            while not self.env.is_finished():
                # ACTOR: Get next action
                action = self.act.next_action(state, valid_actions)

                # ENVIRONMENT: Do next action and receive reinforcement, save state in list
                from_state, action, reinforcement, state, valid_actions, terminal = self.env.step(action)

                # ACTOR: Get next action and update eligibility
                self.act.set_eligibility(from_state, action)

                # CRITIC: Calculate TD-error and update eligibility
                td_error = self.crt.td_error(reinforcement, from_state, state, terminal)
                self.crt.set_eligibility(from_state)

                # Adjustments for all state-action-pairs
                self.crt.adjust(td_error)
                self.act.adjust(td_error)

                # Store data for later visualisation
                i += 1
                self.env.store_training_metadata(current_episode=episode, last_episode=last_episode, current_step=i, state=state)

            loss = self.crt.learn()
            print(f"Episode: {episode}, steps: {i}, NN-loss: {loss}")

        self.env.replay(saps=self.act.get_saps(), values=self.crt.get_values())

if __name__ == '__main__':

    game = GameType.CART_POLE
    critic = CriticType.TABLE

    random.setstate((3, (2147483648, 2337909151, 508968313, 407239879, 2229875462, 2688226694, 3749436469, 3909596653, 2493305755, 511823746, 555896638, 801328813, 4021445285, 1149677343, 2203400059, 3857165114, 2763352496, 532066853, 3694326131, 4017481513, 1347852469, 3667445585, 709417153, 3347580576, 2231762336, 3393824579, 2332021821, 2284525208, 3616205454, 1086750550, 2916213504, 1706754876, 128404664, 641434767, 975049026, 379332160, 2931549689, 2495070204, 2654577110, 2678219735, 255581710, 31820547, 392980924, 3319787849, 4000792056, 304431651, 996490017, 668883355, 2857060436, 3689215455, 2145650263, 2118454466, 2384206497, 513257860, 399538590, 1474875751, 2468541274, 2228592662, 875229770, 2523080389, 4281319483, 279928493, 826137880, 3517670470, 1229316431, 1175019190, 3657212377, 3200867905, 2364627310, 606467411, 3730113061, 3037611665, 20613214, 1310025972, 441749033, 3262868029, 1341932127, 2066354152, 1392923218, 311137713, 2657369111, 4175634187, 1729923382, 1628558989, 2661873831, 2566931237, 776139926, 2384710369, 3222294292, 1301910863, 3831389849, 4059836399, 1462678662, 4080268739, 1474811339, 3821790443, 3874958737, 3005219267, 1877828368, 749678942, 2995172410, 2429953618, 3180745483, 766301626, 1279278644, 2222866730, 3471704107, 4060132496, 1315630229, 1190917412, 2289529830, 3081885717, 2228297891, 634140561, 1350702548, 1312081088, 2592728035, 3042060865, 4157111738, 1352482352, 4222087643, 104413855, 227359715, 672192783, 177334351, 2363057897, 2466660518, 1977949343, 2003974554, 4110350975, 3730773066, 2927154655, 2927191295, 3636161829, 55493899, 4182445629, 3307531249, 1983763064, 114744630, 4058491773, 185584231, 2237771614, 1657892120, 572924594, 4027538835, 2537510209, 2606106365, 2250486931, 2177438820, 406391411, 2476262143, 1778445476, 4050494903, 4264635582, 1081268808, 1802168300, 3639727592, 444318908, 1198345946, 1345023877, 308233290, 1142138354, 4278906071, 55591924, 4264105251, 2027292037, 3157070722, 1412486426, 3977075389, 1578957296, 1996870878, 1100895469, 1022748075, 1521567810, 2451471830, 65266813, 249630047, 3422045701, 3608040715, 134029234, 545394958, 538422763, 3008109314, 3481835519, 2855627893, 1660569260, 2483980718, 741417259, 2195859234, 3126594562, 890696411, 3051562703, 2247397356, 1029560529, 50549415, 2741958033, 2518405563, 2979176493, 798621353, 1384495772, 136469171, 3106369243, 1719329291, 2352601298, 2759501985, 3148063392, 2037124701, 2900763724, 557289149, 2307026386, 2472976309, 2567027250, 3190589464, 2427497177, 2481977333, 2604698738, 1835576351, 2141491829, 3968031486, 2521590817, 3947644798, 3975130063, 2327646560, 654355218, 1641088264, 3869544968, 551531205, 3544858517, 3878372918, 834076806, 3526747496, 3881221507, 3783802393, 3820358633, 3782867009, 3536042293, 3756550433, 3807621759, 1090365974, 605004384, 3586283215, 3753082975, 2879262628, 1763378444, 2394575706, 3643129579, 1521671941, 1181446157, 3208096362, 1026016487, 2563800207, 1031472332, 3382768499, 3732306507, 2146645073, 299512807, 1573197238, 170750066, 72647764, 3981146685, 181694329, 863201387, 135363794, 756021174, 1219091325, 2304066381, 3415534643, 625369843, 846776228, 3356122717, 4075035762, 3079043509, 3797769084, 1278218056, 2198015335, 1634364810, 802098597, 3290971400, 3783409865, 2402439153, 2280530419, 2472868505, 781434899, 2924894640, 2754210807, 288345948, 311357131, 2334804961, 1192708124, 3953152574, 3320485917, 2562778909, 663798572, 462220198, 4435214, 2119744441, 1291719866, 2945127666, 2396118260, 2011135598, 4172863548, 3927918434, 2738673242, 933099701, 1133072109, 3265004152, 3397462708, 3367045191, 2549635677, 2045184673, 350305633, 1035870412, 1143227096, 1247084123, 1367626345, 1920435434, 1788668045, 3030950902, 3564276646, 626786479, 697826725, 3245092750, 195004378, 101610797, 2332029147, 1817580782, 2934473928, 3506595198, 131059118, 3221906356, 1324730909, 4215254252, 2729306814, 1691302307, 2020282049, 3320344338, 564607207, 3908321728, 3330494887, 723680431, 3627678099, 3875472105, 777102887, 583823081, 2565211828, 1835995309, 2223344223, 351324382, 3916768060, 2849898620, 4043100499, 2867041868, 23143359, 2304637966, 1144623807, 1979547134, 1877111483, 3747139259, 1074786278, 3705666155, 3146712807, 3153500848, 749038005, 3354655041, 2259998163, 2933639571, 219397519, 2035217014, 2982684960, 2303793633, 3781582446, 1915020408, 3151354647, 519793356, 1726028964, 2598546094, 3832529980, 706822442, 2868409717, 595577383, 3860257064, 519151037, 394197591, 1966788590, 1401852844, 2202218575, 2397434718, 1089097535, 3615547694, 3502086264, 2294120818, 2490690383, 1155752521, 1587141930, 66148547, 606153932, 3339434046, 664012828, 3017343027, 3610869413, 1471819770, 2671021281, 229277428, 4115829294, 2935421057, 2447967017, 2942218862, 3405203469, 3634990566, 1527239332, 2003341059, 1305211852, 3211214524, 4206962165, 235077620, 1666840045, 2823365261, 3649644833, 433472747, 1183610442, 2439410844, 271331773, 3020128177, 806237609, 4127772657, 619733377, 246989387, 84653645, 3411461915, 3575096094, 243322641, 1422366028, 4047359045, 1841100726, 3304690261, 1627806386, 1292654559, 1135772954, 1158824882, 645830144, 3386369788, 2450036920, 155886444, 396256862, 71750494, 3982681842, 174801518, 4216824975, 837397269, 683755010, 3190815183, 963256226, 2042772579, 2084940509, 1629071572, 2647748931, 1505389772, 3536564133, 4063240069, 291730772, 3039029248, 2974304892, 542948387, 1032529531, 993566014, 951525087, 2898664427, 732774977, 3035855268, 3238266015, 715862345, 2575177197, 3980209127, 422751125, 824724176, 2826954384, 1974609880, 111044522, 2257245133, 1527758538, 3920888680, 1564345921, 2632899988, 2085116860, 2252017372, 2879210062, 1223600081, 752995038, 627047883, 3514403245, 1625407472, 946511732, 613587930, 3240496718, 3116558554, 901438295, 3452139069, 2174042110, 2426138276, 1130397095, 3624877030, 2142210940, 793679849, 3413302819, 2710906899, 1643628724, 3634765911, 1109930365, 1956547118, 3286682247, 1931572107, 2884314029, 2943912145, 2838234096, 4092949622, 2090290045, 493647675, 3729077361, 229198723, 875264361, 3943354302, 3138829616, 354434059, 3268785787, 3054490033, 1437750395, 595822760, 1486534192, 1156668738, 2424994043, 3570589414, 2450800686, 993775872, 2732311039, 1169325501, 3438163636, 833813042, 1332696501, 1264796265, 3909616588, 1619266317, 927825411, 711826571, 258194759, 1556850325, 3236649276, 1597764128, 2651380204, 2091646786, 3520876713, 2666466426, 1899835936, 3380793521, 3698852160, 1451744507, 2173813610, 382634996, 237323196, 899804702, 996499396, 2542059754, 3638488252, 1228865299, 3048911688, 3852392842, 938961987, 3970390187, 216478280, 846179338, 3214523879, 50950454, 4091245242, 349737332, 2442513831, 426432397, 453928285, 1107156630, 2726668209, 2303086553, 3415621412, 4292787406, 684135292, 3219913679, 1508934124, 2993972840, 1016034507, 2174927347, 2937739632, 2879656199, 4204298621, 3777004647, 301557920, 1253323804, 257698012, 1066825627, 3191422725, 4083606412, 2232212268, 546738199, 2430639040, 4062854588, 3831918768, 2799408809, 1707526174, 1491082086, 2210133814, 797287565, 176335675, 1029073943, 2370299016, 2305735669, 2961142015, 822559588, 1219139835, 3795086029, 3938298837, 907700520, 676929704, 388727436, 3440273795, 3567358995, 1582241860, 825355632, 662870329, 624), None))

    if game == GameType.CART_POLE:
        n_episodes = 500
        environment = CartPole(pole_length=0.5, pole_mass=0.1, gravity=-9.8, time_step=0.02, buckets=(5, 5, 6, 6))
        actor = Actor(
            discount_factor=0.9,
            trace_decay=0.9,
            learning_rate=0.2,
            epsilon=DecayingVariable(
                start=0.5,
                end=0.01,
                #episodes=n_episodes,
                linear=True,
                episodes_end_value=0
            ),
        )

        if critic == critic.NEURAL_NETWORK:
            critic = NeuralNetworkCritic(
                discount_factor=0.7,
                learning_rate=0.003,
                input_size=environment.input_space(),
                hidden_size=(32, 32)
            )
        else:
            critic = TableCritic(
                discount_factor=0.9,
                trace_decay=0.9,
                learning_rate=0.3
            )

    elif game == GameType.TOWER_OF_HANOI:
        n_episodes = 500
        environment = TowerOfHanoi(num_pegs=5, num_discs=4)
        actor = Actor(
            discount_factor=0.95,
            trace_decay=0.5,
            learning_rate=0.4,
            epsilon=DecayingVariable(
                start=1,
                end=0.001,
                episodes=n_episodes-25,
                linear=False
            )
        )

        if critic == CriticType.NEURAL_NETWORK:
            critic = NeuralNetworkCritic(
                discount_factor=0.95,
                learning_rate=0.03,
                input_size=environment.input_space(),
                hidden_size=(32, 32)
            )
        else:
            critic = TableCritic(
                discount_factor=0.95,
                trace_decay=0.5,
                learning_rate=0.1
        )

    else:
        n_episodes = 10000
        environment = TheGambler(win_probability=0.4, state_space=100)
        actor = Actor(
            discount_factor=0.95,
            trace_decay=0.2,
            learning_rate=0.4,
            epsilon=DecayingVariable(
                start=1,
                end=0.01,
                episodes=n_episodes,
                linear=False
            ),
        )

        if critic == CriticType.NEURAL_NETWORK:
            critic = NeuralNetworkCritic(
                discount_factor=1,
                learning_rate=0.01,
                input_size=environment.input_space(),
                hidden_size=(32, 32)
            )
        else:
            critic = TableCritic(
                discount_factor=0.95,
                trace_decay=0.2,
                learning_rate=0.01
            )

    agent = Agent(env=environment, act=actor, crt=critic)
    agent.actor_critic_model(n_episodes)
